---
title: "MDP_example"
author: "Olga Lazareva"
date: "12/9/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## A discrimination problem

First of all let's generate $X_{i1},...X_{ik_i}$ coming from $D(p_1)...D(p_k)$ and $Y\sim D(p_j)$ where $1\leq j \leq k$. We are assuming that $\alpha_1, ..\alpha_k \sim Pois(\lambda_k)$ with the same support [0,100]. And we would like to minimize misclassification risk $r_i$ of $Y_j$ which is proportional to $P(Y|i) = \prod_{j=1}^n\frac{\alpha_{i,j}^{'(k_j)}}{M^{(n)}}$,
where (here goes definition of alpha and so on)

```{r}
k = 10 #number of samples X from different distributions we consider
lambda =seq(1,100,length.out = k)
M = 50
theta = seq(1,100,length= 100) #support
n = length(theta)
fun1 = function(n,lambda)(rpois(n,lambda))
sethuraman.cost <- function(fun,mu, M){
  n <- 5000
  y <- fun(n,mu)
  thet <- rbeta(n,shape1 = 1, shape2 = M)
  prob <- rep(0,n)
  prob[1] <- thet[1]
  prob[2:n] = sapply(2:n, function(i) thet[i] * prod(1 - thet[1:(i-1)]))
  return(list('alpha'=y,'pi'=prob))
}

X = matrix(NA, n,k)
j = sample(seq(2,k-2),1) #random choice of distribution for y
for (i in 1:k){
  obj= sethuraman.cost(fun1,mu = lambda[i],M = M)
  X[,i] <- sample(obj$alpha,size= n, prob=obj$pi,replace=T)
  if (i == j)(obj_y = obj)
}  
y <- sample(obj_y$alpha,size= n, prob=obj_y$pi,replace=T)

```

Let's compare disribution of $X_{j-1},X_{j},X_{j+1}$ and $Y$:

```{r}
plot(density(X[,j]),col = "blue",lwd =2,
     xlim=range(min(X[,j-1]),max(X[,j+1])),
     ylim=range(0,max(c(max(density(X[,j-1])$y),max(density(X[,j])$y),max(density(X[,j+1])$y)))),
                main = "Samples distributions")
lines(density(X[,j-1]),col = "skyblue1",lwd =2)
lines(density(X[,j+1]),col = "skyblue1",lwd =2)
lines(density(y),lwd = 3)
legend("topright", 
       c("X(j)","X(j-1)","X(j+1)","Y"), col = c("blue", "skyblue1", "skyblue1","black"),
       lwd = 2)

```

Now we can compute risk $r_i$ for each of the distributions:
```{r}

C = matrix(NA,n,k)
k_ = rep(NA,length.out = length(theta))
k_ = sapply(1:length(theta), function(i) sum(y==i)) #number of Y_i = i
for (i in 1:k){
  C[,i] = sapply(1:length(theta), function(m) sum(X[,i] == m))
}
alpha = matrix(NA,length(theta),k)
for (l in 1:k){
  alpha[,l] = dpois(theta,lambda[l])*M
  alpha[,l]+C[,l] #alpha'
}

poli = function(x,n){
  p = x
  for (i in 1:(n-1)){
    p = prod(p,x+i)
  }
  if (n ==0)(p = 1)
  return(p)
}
probs = matrix(NA,length(theta),length(lambda)) #risk
for (i in theta){
  for (l in 1:k){
    probs[i,l] = log(poli(alpha[i,l],k_[i])/poli(M,length(theta)))
  }
}
probs = sapply(1:length(lambda),function(i)prod(probs[,i]))
plot(probs,ylab = "log(risk)", xlab = "model",col = "blue", lwd = 4)
abline(v = j, lwd=3, lty=2)
points(x = which.min(probs),y = probs[which.min(probs)], col = "red", lwd = 3)
legend("topright", 
       c("risk_i","j","minimum risk"), col = c("blue", "black","red"),
       lwd = 2)


```

As we see, the minimum value of the risk correspondes to the case where $i =j$ which means that the distribution of $Y$ was classified correctly
